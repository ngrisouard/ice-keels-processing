{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d3e9b4-bfb2-4d72-9a09-7f5fc12c7f9a",
   "metadata": {},
   "source": [
    "# Writing all useful output from the simulations into one netcCDF archive\n",
    "\n",
    "## General architecture\n",
    "\n",
    "For each simulations (F05H05, etc), we need the following:\n",
    "* Variables: \n",
    "    1. horizontally averaged integrand of the flux $\\Phi$ for the upstream:\n",
    "        $$\\phi_{U}(z, t) = \\frac{1}{\\mu}\\frac{g}{\\rho_1}\\int_{U}\\frac{|\\nabla b|^2}{\\partial_z\\rho_*}\\frac{\\text d x}{L_U(z)},$$\n",
    "        where I did *not* average in time, and where $L_{U}(x)$ is the length of the integration path at a given depth, which depends on $z$ because of the topography\n",
    "    2. Vertical sorted buyoancy gradient in the upstream $N_{*, U}^2(z, t)$. Note that as we discussed, it should *not* depend on $x$, ever. If it does, then the sorted buoyancy needs to be binned in $x$ to match the dimension of $z$, I believe. Note that I'm not 100% sure how it should be done because I've never done it myself, but it should not be a function of $x$.\n",
    "    2. $L_U(z)$, because we will probably need it later\n",
    "    3. $\\phi_D(z, t)$, same as $\\phi_U$ but for downstream \n",
    "    4. $N_{*, D}(z, t)$, same as $N_{*, U}(z, t)$ but for downstream \n",
    "    5. $L_D(z)$, same as $L_U(z)$ but for downstream\n",
    "    6. Time\n",
    "    7. $z$ (same for every simulation but it is simpler that way)\n",
    "* Dimensions:\n",
    "    1. Time\n",
    "    2. $z$\n",
    "    \n",
    "Each simulation's data will be in a group that is named after the simulations name (e.g., `F05H09`).\n",
    "\n",
    "## Preparing Data\n",
    "\n",
    "The lines below are me, creating fake data to make the procedure work. You do not need to format your data in the same way, you just need to have the right inputs for the `create_group` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6dd8918-2820-4042-a458-d4edf0c15ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "etas = [0.5, 0.95, 1.2, 2.]  # list of non-dim drafts\n",
    "Frs = [0.5, 1., 1.5, 2.]  # list of Froude numbers\n",
    "\n",
    "# Create empty variables, later to be filled with simulation data \n",
    "simIDs = []  # empty list that will contain all simulation names\n",
    "# Dictionaries: the key will be e.g. F05H09, filled with corresponding data\n",
    "time = {}  # empty dictionnary for the time array of each simulation\n",
    "LUs = {}\n",
    "LDs = {}\n",
    "pUs = {}\n",
    "pDs = {}\n",
    "NUs = {}\n",
    "NDs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a71b07-18fe-408f-a047-625a93b43c85",
   "metadata": {},
   "source": [
    "Below, I create a set of fake data. I will use the same for each simulation, but in the final version, you would load the simulations' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4fcbb9a-678a-49fd-91e0-cf20dacc2eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho1 = 1000  # [kg/m3] or whatever the actual value is\n",
    "g = 9.81  # [m/s2] idem\n",
    "mu = 2e-3  # [m2/s] salt diffusivity\n",
    "\n",
    "depth = np.linspace(0., 80., 640)  # fake z array; \n",
    "# The values might be correct but use the simulation's array anyway\n",
    "FakeTime = np.linspace(0., 132., 300)\n",
    "FakeLU = 1*depth  # To have the same size as z\n",
    "FakeLD = 1*depth\n",
    "FakephiU = np.meshgrid(FakeTime, depth)  # again to have correct size\n",
    "FakephiD = np.meshgrid(FakeTime, depth)  # again to have correct size\n",
    "FakeNU = np.meshgrid(FakeTime, depth)  # again to have correct size\n",
    "FakeND = np.meshgrid(FakeTime, depth)  # again to have correct size\n",
    "\n",
    "for H in etas:\n",
    "    for F in Frs:\n",
    "        simID = \"F{0:02d}H{1:02d}\".format(int(F*10), int(H*10))  # F05H05, etc\n",
    "        simIDs.append(simID)\n",
    "        time[simID] = FakeTime\n",
    "        LUs[simID] = FakeLU\n",
    "        LDs[simID] = FakeLD\n",
    "        pUs[simID] = FakeLU\n",
    "        pDs[simID] = FakeLD\n",
    "        NUs[simID] = FakeLU + 1. # +1 because I don't want zeros\n",
    "        NDs[simID] = FakeLD + 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd39dec3-b7eb-4427-bc39-0ac4bb8363b3",
   "metadata": {},
   "source": [
    "## Wrapping the data into a netCDF archive\n",
    "\n",
    "Once you replaced the cell above with one that pupulates the dictionaries with actual data, you should be able to use the rest of the code as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "855114f3-995a-4a6c-a354-d13733b97663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'netCDF4._netCDF4.Dataset'>\n",
      "root group (NETCDF4 data model, file format HDF5):\n",
      "    title: Mixing under ice keels\n",
      "    dimensions(sizes): \n",
      "    variables(dimensions): \n",
      "    groups: F05H05, F10H05, F15H05, F20H05, F05H09, F10H09, F15H09, F20H09, F05H12, F10H12, F15H12, F20H12, F05H20, F10H20, F15H20, F20H20\n"
     ]
    }
   ],
   "source": [
    "from create_ice_keels_netcdf import *\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"ice-keels.nc\"):  # we start from scratch\n",
    "    os.remove(\"ice-keels.nc\")\n",
    "else:\n",
    "    print(\"The file does not exist\")\n",
    "\n",
    "create_netcdf(simIDs, time, depth, LUs, LDs, pUs, pDs, NUs, NDs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
